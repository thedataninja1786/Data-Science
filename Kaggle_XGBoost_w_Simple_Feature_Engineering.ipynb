{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle- XGBoost w/ Simple Feature Engineering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIejyMKVrAk12YRSDCml2H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedataninja1786/Data-Science/blob/main/Kaggle_XGBoost_w_Simple_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH4c2RPiZYM_"
      },
      "source": [
        "# Importing the necessary modules \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "import pandas as pd \n",
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikLNlQB0ZgwL"
      },
      "source": [
        "train_data = pd.read_csv('/train.csv')\n",
        "test_data = pd.read_csv('/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTcqDYJ7Znvr"
      },
      "source": [
        "# Concatenate the train and test dataframes so the preprocessing is applied to both \n",
        "full_data = pd.concat([train_data,test_data]).reset_index(drop = True)\n",
        "\n",
        "sale_price = train_data['SalePrice'].reset_index(drop=True)\n",
        "# Remove the Sale Price dependent variable from the combined dataset \n",
        "del full_data['SalePrice']\n",
        "\n",
        "print(f'Train dataframe contains {train_data.shape[0]} rows and {train_data.shape[1]} columns.\\n')\n",
        "print(f'Test dataframe contains {test_data.shape[0]} rows and {test_data.shape[1]} columns.\\n')\n",
        "print(f'The merged dataframe contains {full_data.shape[0]} rows and {full_data.shape[1]} columns.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo7Rdf0nZqsF"
      },
      "source": [
        "# Drop columns with more than 45% missing data \n",
        "cols_to_drop = []\n",
        "for column in full_data:\n",
        "  if full_data[column].isnull().sum() / len(full_data) >= 0.4:\n",
        "    cols_to_drop.append(column)\n",
        "full_data.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "print(f'{len(cols_to_drop)} columns dropped, the full dataset now comprises of {full_data.shape[1]} variables.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTA-fwkDZsxW"
      },
      "source": [
        "# Replacing the NA values with the median for the numerical \n",
        "# columns and scaling the data\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "columns = full_data.columns.values\n",
        "for column in columns:\n",
        "  if full_data[column].dtype == np.int64 or full_data[column].dtype == np.float64:\n",
        "    full_data[column] = full_data[column].fillna(full_data[column].median())\n",
        "    full_data[column] = scaler.fit_transform(np.array(full_data[column]).reshape(-1,1))\n",
        "\n",
        "# Print the updated data  \n",
        "full_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2f_aIQEZ024"
      },
      "source": [
        "# Calculate the correlation of the numerical variables with the Sale Price \n",
        "# Use the training dataset that inludes the Sale Price variable  \n",
        "\n",
        "corr = train_data.corr()\n",
        "plt.subplots(figsize=(19,10))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(corr, vmax=0.7, cmap = cmap, square=True)\n",
        "\n",
        "cols_to_drop = []\n",
        "# Get the correlation of the dependent variable with the rest of the features\n",
        "sale_price_corr = train_data.corr()['SalePrice'][:-1] \n",
        "\n",
        "# Drop all the columns from the full data that correlate < |0.12| with the sale price, \n",
        "# since will add little value to the model \n",
        "for column,row in sale_price_corr.iteritems():\n",
        "  if abs(float(row)) < 0.12:\n",
        "    cols_to_drop.append(column)\n",
        "full_data.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "print(f'{len(cols_to_drop)} columns dropped, the full dataset now comprises\\ of {full_data.shape[1]} variables.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qiP017yZ-DS"
      },
      "source": [
        "# Drop the columns that have > 6 unique categorical classes\n",
        "count = 0 \n",
        "columns = full_data.columns.values\n",
        "for column in columns:\n",
        "  if full_data[column].dtype not in (np.int64, np.float64) and full_data[column].nunique() > 6:\n",
        "    count += 1 \n",
        "    full_data.drop(column, axis = 1, inplace = True)\n",
        "\n",
        "print(f'{count} columns dropped, the full dataset now comprises of {full_data.shape[1]} variables.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5nJkUyHaANM"
      },
      "source": [
        "# Replace nas with the most frequent occurring value in the categorical data \n",
        "full_data = full_data.fillna(full_data.mode().iloc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhKkTd7IaEX9"
      },
      "source": [
        "# Label / one-hot encode the categorical variables\n",
        "# One-hot encode the columns that have > 2 categorical variables\n",
        "# Label-encode the columns that have only 2 categorical variables \n",
        "\n",
        "# Instanciating the labelencoder\n",
        "labelencoder = LabelEncoder()\n",
        "cols_to_drop = []\n",
        "\n",
        "columns = full_data.columns.values\n",
        "for column in columns:\n",
        "    if full_data[column].dtype not in (np.int64, np.float64) and full_data[column].nunique() > 2: \n",
        "      dummies = pd.get_dummies(full_data[column], prefix = str(column))\n",
        "      cols_to_drop.append(column)\n",
        "      full_data = pd.concat([full_data, dummies], axis = 1)\n",
        "    elif full_data[column].dtype not in (np.int64, np.float64) and full_data[column].nunique() < 3: \n",
        "      full_data[column] = labelencoder.fit_transform(full_data[column])\n",
        "      cols_to_drop.append(column)\n",
        "\n",
        "full_data.drop(cols_to_drop, axis = 1, inplace = True)\n",
        "print(f'The new dataframe comprises of {test_data.shape[0]} rows and {test_data.shape[1]} columns.\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGN1T5JVaH_y"
      },
      "source": [
        "#Now that the data have been processes split again into train and test \n",
        "train_df = full_data[:train_data.shape[0]]\n",
        "test_df =  full_data[train_data.shape[0]:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmQBE-5aJi6"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgboost = XGBRegressor(learning_rate=0.008,\n",
        "                       n_estimators=6000,\n",
        "                       max_depth=8,\n",
        "                       min_child_weight=0,\n",
        "                       gamma=0.6,\n",
        "                       subsample=0.7,\n",
        "                       colsample_bytree=0.7,\n",
        "                       objective='reg:squarederror',\n",
        "                       nthread=-1,\n",
        "                       scale_pos_weight=1,\n",
        "                       seed=27,\n",
        "                       reg_alpha=0.00006,\n",
        "                       random_state=42)\n",
        "\n",
        "xgb = xgboost.fit(train_df,sale_price)\n",
        "training_accuracy = xgb.score(train_df,sale_price)\n",
        "print(\"Training accuracy: %.2f%%\" % (training_accuracy * 100.0))\n",
        "xgb_predictions = xgb.predict(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drjKj79HaPo1"
      },
      "source": [
        "submission = pd.DataFrame({'ID':test_data['Id'],'SalePrice':xgb_predictions})\n",
        "submission.to_csv('submission.csv',index = False)\n",
        "submission"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}