{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  Titanic | Top 1% w/ Simple Feature Engineering",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedataninja1786/Data-Science/blob/main/Copy_of_Titanic_%7C_Top_1_w_Simple_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOcFVfhHjEh9"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import itertools\n",
        "import random \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfpVHNrzrqce"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/titanic/train.csv')\n",
        "test_df =  pd.read_csv('/content/drive/MyDrive/titanic/test.csv')\n",
        "\n",
        "# Join the train and test dataframes so the data preprocessing \n",
        "# will be done simultenously in both datasets \n",
        "full = train_df.append(test_df, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSrXxu0XjMLA"
      },
      "source": [
        "def data_preprocessing(df):\n",
        "  \n",
        "  df['Sex'] = df['Sex'].replace(['male'],0)\n",
        "  df['Sex'] = df['Sex'].replace(['female'],1)\n",
        "  df['title'] = np.NaN\n",
        "  full['alone'] = np.NaN\n",
        "  df['cabin_class'] = np.NaN\n",
        "\n",
        "  # Identify if a passenger is alone in the ship \n",
        "  for i,_ in enumerate(df['alone']):\n",
        "    if df['SibSp'][i] + df['Parch'][i] == 0:\n",
        "      df['alone'][i] = 1\n",
        "    else:\n",
        "      df['alone'][i] = 0 \n",
        "\n",
        "  # Handle missing values\n",
        "  cols = ['SibSp','Parch','Fare','Age']\n",
        "  for col in cols:\n",
        "    df[col].fillna(df[col].median(), inplace = True)\n",
        "\n",
        "  # Feature-engineer the cabin \n",
        "  for i,row in enumerate(df['Cabin']):\n",
        "    # Get cabin \n",
        "    df['cabin_class'][i] =  str(row)[:1]\n",
        "\n",
        "  # Cabin distribution where available \n",
        "  cabin_distribution = {}\n",
        "  count = 0 \n",
        "  for row in df['cabin_class']:\n",
        "    if row != 'n':\n",
        "      count += 1 \n",
        "      if row not in cabin_distribution:\n",
        "        cabin_distribution[row] = 1 \n",
        "      else:\n",
        "        cabin_distribution[row] +=1 \n",
        "\n",
        "  # Calculate the probability of being in a sepcific cabin class  \n",
        "  cabin_pdf = {k:v / count for k, v in cabin_distribution.items()}\n",
        "\n",
        "  # Calculate the cumulative probability of being in a specific cabin class \n",
        "  keys, vals = cabin_distribution.keys(), cabin_pdf.values()\n",
        "  cabin_cdf = dict(zip(keys, itertools.accumulate(vals)))\n",
        "  cabin_cdf = sorted(cabin_cdf.items(), key=lambda x: x[1])    \n",
        "\n",
        "  # Assign randomly cabin-sections to passengers that are missing the cabin \n",
        "  # field, based on the probabilities calculated above \n",
        "  for i,row in enumerate(df['cabin_class']):\n",
        "    random_num = random.random()\n",
        "    if row == 'n':\n",
        "      if random_num < cabin_cdf[0][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[0][0]\n",
        "      elif cabin_cdf[0][1] <= random_num < cabin_cdf[1][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[1][0]\n",
        "\n",
        "      elif cabin_cdf[1][1] <= random_num < cabin_cdf[2][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[2][0]\n",
        "      \n",
        "      elif cabin_cdf[2][1] <= random_num < cabin_cdf[3][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[2][0]\n",
        "\n",
        "      elif cabin_cdf[3][1] <= random_num < cabin_cdf[4][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[3][0]\n",
        "\n",
        "      elif cabin_cdf[3][1] <= random_num < cabin_cdf[4][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[4][0]\n",
        "\n",
        "      elif cabin_cdf[4][1] <= random_num < cabin_cdf[5][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[4][0]\n",
        "      \n",
        "      elif cabin_cdf[5][1] <= random_num < cabin_cdf[6][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[5][0]\n",
        "\n",
        "      elif cabin_cdf[6][1] <= random_num < cabin_cdf[7][1]:\n",
        "        df['cabin_class'][i] =  cabin_cdf[6][0]\n",
        "      else:\n",
        "        df['cabin_class'][i] = cabin_cdf[7][0]\n",
        "\n",
        "  # Perform feature engineering to obtain additional title-info \n",
        "  for i,row in enumerate(df['Name']):\n",
        "    # Get person's title \n",
        "    df['title'][i] = row.split(',')[1].split('.')[0]\n",
        "  \n",
        "\n",
        "  # Embarked one-hot encoding \n",
        "  embarked_dummies = pd.get_dummies(df.Embarked, prefix='Embarked')\n",
        "  df = pd.concat([df, embarked_dummies], axis=1)\n",
        "\n",
        "  # Person's title one-hot encoding \n",
        "  title_dummies = pd.get_dummies(df.title, prefix='title')\n",
        "  df = pd.concat([df, title_dummies], axis=1)\n",
        "\n",
        "  # Cabin class one-hot encoding \n",
        "  cabin_class_dummies = pd.get_dummies(df.cabin_class, prefix = 'cabin_class')\n",
        "  df = pd.concat([df, cabin_class_dummies], axis = 1)\n",
        "\n",
        "\n",
        "  #Remove unecessary columns \n",
        "  del df['Name']\n",
        "  del df['PassengerId']\n",
        "  del df['title']\n",
        "  del df['Embarked']\n",
        "  del df['Cabin']\n",
        "  del df['Ticket']\n",
        "  del df['cabin_class']\n",
        "\n",
        "  return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuEcCyFlONHF"
      },
      "source": [
        "# Preprocess the data and create the train / test sets \n",
        "full = data_preprocessing(full)\n",
        "X_train = full[:891]\n",
        "y_train = full['Survived'][:891]\n",
        "X_test = full[891:]\n",
        "del X_train['Survived']\n",
        "del X_test['Survived']\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmXAUi6QlWPi"
      },
      "source": [
        "# Stack two models for higher accuracy  \n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LR = LogisticRegression(max_iter=1000,C=0.175,random_state=42)\n",
        "LR.fit(X_train, y_train)\n",
        "lr_training_accuracy = LR.score(X_train, y_train)\n",
        "predictions = LR.predict(X_test)\n",
        "lr_predictions = [int(x) for x in predictions]\n",
        "\n",
        "xgboost = XGBRegressor(learning_rate=0.005,\n",
        "                       n_estimators=6000,\n",
        "                       max_depth=4,\n",
        "                       min_child_weight=0,\n",
        "                       gamma=0.6,\n",
        "                       subsample=0.7,\n",
        "                       colsample_bytree=0.7,\n",
        "                       objective='reg:squarederror',\n",
        "                       nthread=-1,\n",
        "                       scale_pos_weight=1,\n",
        "                       seed=27,\n",
        "                       reg_alpha=0.00006,\n",
        "                       random_state=42)\n",
        "\n",
        "xgb = xgboost.fit(X_train,y_train)\n",
        "xgb_training_accuracy = xgb.score(X_train,y_train)\n",
        "\n",
        "xgb_predictions = xgb.predict(X_test)\n",
        "xgb_predictions = [round(x) for x in xgb_predictions]\n",
        "\n",
        "\n",
        "print(\"Logistic Regression training accuracy: %.2f%%\" % (lr_training_accuracy * 100.0))\n",
        "print(\"\\nXGB training accuracy: %.2f%%\" % (xgb_training_accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU76HhvBv_Q2"
      },
      "source": [
        "# Combine the results from both models  \n",
        "predictions = [round((lr_pred + xgb_pred) / 2) for lr_pred,xgb_pred in zip(lr_predictions,xgb_predictions)]\n",
        "# Create submission file \n",
        "submission = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':predictions})\n",
        "submission.to_csv('submission.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}